{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxwe\\AppData\\Local\\Temp\\ipykernel_5144\\691236561.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scripts.datacleaningutils import datacleaning\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to our data cleaning portion. In this notebook we will\n",
    "1. Find out if there are missing data\n",
    "2. Explain how we clean our missing data (the different imputation methods)\n",
    "### EDA will be done in a different notebook\n",
    "### Let's begin by importing all our data, we will take all data into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>PRES</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>RAIN</th>\n",
       "      <th>wd</th>\n",
       "      <th>WSPM</th>\n",
       "      <th>station</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1024.5</td>\n",
       "      <td>-21.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>5.7</td>\n",
       "      <td>Dongsi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>1025.1</td>\n",
       "      <td>-22.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Dongsi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>1025.3</td>\n",
       "      <td>-24.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>5.3</td>\n",
       "      <td>Dongsi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>1026.2</td>\n",
       "      <td>-25.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>4.9</td>\n",
       "      <td>Dongsi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>1027.1</td>\n",
       "      <td>-24.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>3.2</td>\n",
       "      <td>Dongsi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>35060</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>16.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1013.5</td>\n",
       "      <td>-16.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Dongsi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>35061</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>18.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>1013.6</td>\n",
       "      <td>-15.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WNW</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Dongsi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>35062</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>23.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1014.2</td>\n",
       "      <td>-13.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Dongsi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>35063</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>23.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1014.4</td>\n",
       "      <td>-12.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Dongsi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35063</th>\n",
       "      <td>35064</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>30.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1014.1</td>\n",
       "      <td>-15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNE</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Dongsi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35064 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          No  year  month  day  hour  PM2.5  PM10   SO2   NO2      CO    O3  \\\n",
       "0          1  2013      3    1     0    9.0   9.0   3.0  17.0   300.0  89.0   \n",
       "1          2  2013      3    1     1    4.0   4.0   3.0  16.0   300.0  88.0   \n",
       "2          3  2013      3    1     2    7.0   7.0   NaN  17.0   300.0  60.0   \n",
       "3          4  2013      3    1     3    3.0   3.0   5.0  18.0     NaN   NaN   \n",
       "4          5  2013      3    1     4    3.0   3.0   7.0   NaN   200.0  84.0   \n",
       "...      ...   ...    ...  ...   ...    ...   ...   ...   ...     ...   ...   \n",
       "35059  35060  2017      2   28    19   16.0  51.0   3.0  29.0   400.0  73.0   \n",
       "35060  35061  2017      2   28    20   18.0  45.0   3.0  43.0   500.0  54.0   \n",
       "35061  35062  2017      2   28    21   23.0  58.0   5.0  61.0   700.0  28.0   \n",
       "35062  35063  2017      2   28    22   23.0  53.0   9.0  75.0   900.0  15.0   \n",
       "35063  35064  2017      2   28    23   30.0  71.0  11.0  87.0  1200.0   4.0   \n",
       "\n",
       "       TEMP    PRES  DEWP  RAIN   wd  WSPM station  \n",
       "0      -0.5  1024.5 -21.4   0.0  NNW   5.7  Dongsi  \n",
       "1      -0.7  1025.1 -22.1   0.0   NW   3.9  Dongsi  \n",
       "2      -1.2  1025.3 -24.6   0.0  NNW   5.3  Dongsi  \n",
       "3      -1.4  1026.2 -25.5   0.0    N   4.9  Dongsi  \n",
       "4      -1.9  1027.1 -24.5   0.0  NNW   3.2  Dongsi  \n",
       "...     ...     ...   ...   ...  ...   ...     ...  \n",
       "35059  12.5  1013.5 -16.2   0.0   NW   2.4  Dongsi  \n",
       "35060  11.6  1013.6 -15.1   0.0  WNW   0.9  Dongsi  \n",
       "35061  10.8  1014.2 -13.3   0.0   NW   1.1  Dongsi  \n",
       "35062  10.5  1014.4 -12.9   0.0  NNW   1.2  Dongsi  \n",
       "35063   8.6  1014.1 -15.9   0.0  NNE   1.3  Dongsi  \n",
       "\n",
       "[35064 rows x 18 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### For testing purposes, we will take a look at 1 dataframe as reference, before proceeding\n",
    "df_test = pd.read_csv(\"data\\PRSA_Data_Dongsi_20130301-20170228.csv\")\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRSA_Data_Aotizhongxin_20130301-20170228.csv',\n",
       " 'PRSA_Data_Changping_20130301-20170228.csv',\n",
       " 'PRSA_Data_Dingling_20130301-20170228.csv',\n",
       " 'PRSA_Data_Dongsi_20130301-20170228.csv',\n",
       " 'PRSA_Data_Guanyuan_20130301-20170228.csv',\n",
       " 'PRSA_Data_Gucheng_20130301-20170228.csv',\n",
       " 'PRSA_Data_Huairou_20130301-20170228.csv',\n",
       " 'PRSA_Data_Nongzhanguan_20130301-20170228.csv',\n",
       " 'PRSA_Data_Shunyi_20130301-20170228.csv',\n",
       " 'PRSA_Data_Tiantan_20130301-20170228.csv',\n",
       " 'PRSA_Data_Wanliu_20130301-20170228.csv',\n",
       " 'PRSA_Data_Wanshouxigong_20130301-20170228.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "files = os.listdir(\"data\")\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then create an array of data frames to store all the data. This will be useful later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arr = []\n",
    "for i in files:\n",
    "    df_arr.append(pd.read_csv(f\"data/{i}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dataframes: 12\n",
      "\n",
      "Number of rows :\n",
      "PRSA_Data_Aotizhongxin_20130301-20170228.csv : 35064\n",
      "PRSA_Data_Changping_20130301-20170228.csv : 35064\n",
      "PRSA_Data_Dingling_20130301-20170228.csv : 35064\n",
      "PRSA_Data_Dongsi_20130301-20170228.csv : 35064\n",
      "PRSA_Data_Guanyuan_20130301-20170228.csv : 35064\n",
      "PRSA_Data_Gucheng_20130301-20170228.csv : 35064\n",
      "PRSA_Data_Huairou_20130301-20170228.csv : 35064\n",
      "PRSA_Data_Nongzhanguan_20130301-20170228.csv : 35064\n",
      "PRSA_Data_Shunyi_20130301-20170228.csv : 35064\n",
      "PRSA_Data_Tiantan_20130301-20170228.csv : 35064\n",
      "PRSA_Data_Wanliu_20130301-20170228.csv : 35064\n",
      "PRSA_Data_Wanshouxigong_20130301-20170228.csv : 35064\n",
      "\n",
      "Number of missing Data :\n",
      "PRSA_Data_Aotizhongxin_20130301-20170228.csv : 1776\n",
      "PRSA_Data_Changping_20130301-20170228.csv : 1521\n",
      "PRSA_Data_Dingling_20130301-20170228.csv : 2012\n",
      "PRSA_Data_Dongsi_20130301-20170228.csv : 3197\n",
      "PRSA_Data_Guanyuan_20130301-20170228.csv : 1753\n",
      "PRSA_Data_Gucheng_20130301-20170228.csv : 1401\n",
      "PRSA_Data_Huairou_20130301-20170228.csv : 1639\n",
      "PRSA_Data_Nongzhanguan_20130301-20170228.csv : 1206\n",
      "PRSA_Data_Shunyi_20130301-20170228.csv : 2178\n",
      "PRSA_Data_Tiantan_20130301-20170228.csv : 1126\n",
      "PRSA_Data_Wanliu_20130301-20170228.csv : 2107\n",
      "PRSA_Data_Wanshouxigong_20130301-20170228.csv : 1297\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of dataframes: {len(df_arr)}\\n\")\n",
    "print(f\"Number of rows :\")\n",
    "for i in range(len(files)):\n",
    "    print(f\"{files[i]} : {len(df_arr[i])}\")\n",
    "print(\"\")\n",
    "print(f\"Number of missing Data :\")\n",
    "for i in range(len(files)):\n",
    "    print(f\"{files[i]} : {(df_arr[i].isnull().sum().max())}\")\n",
    "    # print(\"=================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------\n",
    "### We need to impute the data somehow. We can check for the best locations to impute missing data.\n",
    "### Idea : find district in Beijing 7km away from each in our dataset, and check correlation between columns.\n",
    "### Reason : Correlation != Causation. It doesn't make sense for a district far away from another to have the same weather conditions. It could just be pure coincidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Plot out each district on google maps\n",
    "<img src=\"img/gmap1.png\" alt=\"gmap\" height=\"300\"></img>\n",
    "### Step 2: Identify closest all datasets within 7km of each other (using google measure tool)\n",
    "<img src=\"img/gmap2.png\" alt=\"gmap\" height=\"300\"></img>\n",
    "### We have found Dongsi, Guanyuan, Nongzhanguan and Tiantan to be the closest 4 cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['PRSA_Data_Dongsi_20130301-20170228.csv',\n",
    " 'PRSA_Data_Guanyuan_20130301-20170228.csv',\n",
    " 'PRSA_Data_Nongzhanguan_20130301-20170228.csv',\n",
    " 'PRSA_Data_Tiantan_20130301-20170228.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have written a library to make the data cleaning process easier. We will briefly explain the use of each function, and attach the source code in datacleaningutils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all data\n",
    "df_arr = []\n",
    "for i in names:\n",
    "    df_arr.append(pd.read_csv(f\"data/{i}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell below returns the number of rows that contains missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dataframes: 4\n",
      "\n",
      "Number of rows :\n",
      "PRSA_Data_Dongsi_20130301-20170228.csv : 35064\n",
      "PRSA_Data_Guanyuan_20130301-20170228.csv : 35064\n",
      "PRSA_Data_Nongzhanguan_20130301-20170228.csv : 35064\n",
      "PRSA_Data_Tiantan_20130301-20170228.csv : 35064\n",
      "\n",
      "Number of names Data :\n",
      "PRSA_Data_Dongsi_20130301-20170228.csv : 3197\n",
      "PRSA_Data_Guanyuan_20130301-20170228.csv : 1753\n",
      "PRSA_Data_Nongzhanguan_20130301-20170228.csv : 1206\n",
      "PRSA_Data_Tiantan_20130301-20170228.csv : 1126\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of dataframes: {len(df_arr)}\\n\")\n",
    "print(f\"Number of rows :\")\n",
    "for i in range(len(names)):\n",
    "    print(f\"{names[i]} : {len(df_arr[i])}\")\n",
    "print(\"\")\n",
    "print(f\"Number of names Data :\")\n",
    "for i in range(len(names)):\n",
    "    print(f\"{names[i]} : {(df_arr[i].isnull().sum().max())}\")\n",
    "    # print(\"=================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the other hand, the cell below uses our custom function to identify windows of missing data for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.datacleaningutils import datacleaning\n",
    "cleaner = datacleaning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRSA_Data_Dongsi_20130301-20170228.csv\n",
      "No\n",
      "year\n",
      "month\n",
      "day\n",
      "hour\n",
      "PM2.5\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 214\n",
      "longest window of missing data: 214\n",
      "Point missing data 113\n",
      "PM10\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 179\n",
      "longest window of missing data: 179\n",
      "Point missing data 115\n",
      "SO2\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 204\n",
      "longest window of missing data: 204\n",
      "Point missing data 123\n",
      "NO2\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 204\n",
      "longest window of missing data: 204\n",
      "Point missing data 129\n",
      "CO\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 304\n",
      "longest window of missing data: 304\n",
      "Point missing data 199\n",
      "O3\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 247\n",
      "longest window of missing data: 247\n",
      "Point missing data 164\n",
      "TEMP\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 8\n",
      "longest window of missing data: 8\n",
      "Point missing data 5\n",
      "PRES\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 8\n",
      "longest window of missing data: 8\n",
      "Point missing data 5\n",
      "DEWP\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 8\n",
      "longest window of missing data: 8\n",
      "Point missing data 5\n",
      "RAIN\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 8\n",
      "longest window of missing data: 8\n",
      "Point missing data 5\n",
      "wd\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 60\n",
      "longest window of missing data: 60\n",
      "Point missing data 53\n",
      "WSPM\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 5\n",
      "longest window of missing data: 5\n",
      "Point missing data 2\n",
      "station\n",
      "=======================\n",
      "PRSA_Data_Guanyuan_20130301-20170228.csv\n",
      "No\n",
      "year\n",
      "month\n",
      "day\n",
      "hour\n",
      "PM2.5\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 189\n",
      "longest window of missing data: 189\n",
      "Point missing data 97\n",
      "PM10\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 150\n",
      "longest window of missing data: 150\n",
      "Point missing data 77\n",
      "SO2\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 172\n",
      "longest window of missing data: 172\n",
      "Point missing data 110\n",
      "NO2\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 213\n",
      "longest window of missing data: 213\n",
      "Point missing data 139\n",
      "CO\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 345\n",
      "longest window of missing data: 345\n",
      "Point missing data 222\n",
      "O3\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 291\n",
      "longest window of missing data: 291\n",
      "Point missing data 170\n",
      "TEMP\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 8\n",
      "longest window of missing data: 8\n",
      "Point missing data 5\n",
      "PRES\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 8\n",
      "longest window of missing data: 8\n",
      "Point missing data 5\n",
      "DEWP\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 8\n",
      "longest window of missing data: 8\n",
      "Point missing data 5\n",
      "RAIN\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 8\n",
      "longest window of missing data: 8\n",
      "Point missing data 5\n",
      "wd\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 63\n",
      "longest window of missing data: 63\n",
      "Point missing data 56\n",
      "WSPM\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 5\n",
      "longest window of missing data: 5\n",
      "Point missing data 2\n",
      "station\n",
      "=======================\n",
      "PRSA_Data_Nongzhanguan_20130301-20170228.csv\n",
      "No\n",
      "year\n",
      "month\n",
      "day\n",
      "hour\n",
      "PM2.5\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 253\n",
      "longest window of missing data: 253\n",
      "Point missing data 134\n",
      "PM10\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 193\n",
      "longest window of missing data: 193\n",
      "Point missing data 129\n",
      "SO2\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 172\n",
      "longest window of missing data: 172\n",
      "Point missing data 101\n",
      "NO2\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 245\n",
      "longest window of missing data: 245\n",
      "Point missing data 159\n",
      "CO\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 341\n",
      "longest window of missing data: 341\n",
      "Point missing data 216\n",
      "O3\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 233\n",
      "longest window of missing data: 233\n",
      "Point missing data 158\n",
      "TEMP\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 8\n",
      "longest window of missing data: 8\n",
      "Point missing data 5\n",
      "PRES\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 8\n",
      "longest window of missing data: 8\n",
      "Point missing data 5\n",
      "DEWP\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 8\n",
      "longest window of missing data: 8\n",
      "Point missing data 5\n",
      "RAIN\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 8\n",
      "longest window of missing data: 8\n",
      "Point missing data 5\n",
      "wd\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 60\n",
      "longest window of missing data: 60\n",
      "Point missing data 53\n",
      "WSPM\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 5\n",
      "longest window of missing data: 5\n",
      "Point missing data 2\n",
      "station\n",
      "=======================\n",
      "PRSA_Data_Tiantan_20130301-20170228.csv\n",
      "No\n",
      "year\n",
      "month\n",
      "day\n",
      "hour\n",
      "PM2.5\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 355\n",
      "longest window of missing data: 355\n",
      "Point missing data 235\n",
      "PM10\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 323\n",
      "longest window of missing data: 323\n",
      "Point missing data 227\n",
      "SO2\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 303\n",
      "longest window of missing data: 303\n",
      "Point missing data 220\n",
      "NO2\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 357\n",
      "longest window of missing data: 357\n",
      "Point missing data 261\n",
      "CO\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 467\n",
      "longest window of missing data: 467\n",
      "Point missing data 345\n",
      "O3\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 421\n",
      "longest window of missing data: 421\n",
      "Point missing data 327\n",
      "TEMP\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 8\n",
      "longest window of missing data: 8\n",
      "Point missing data 5\n",
      "PRES\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 8\n",
      "longest window of missing data: 8\n",
      "Point missing data 5\n",
      "DEWP\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 8\n",
      "longest window of missing data: 8\n",
      "Point missing data 5\n",
      "RAIN\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 8\n",
      "longest window of missing data: 8\n",
      "Point missing data 5\n",
      "wd\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 60\n",
      "longest window of missing data: 60\n",
      "Point missing data 53\n",
      "WSPM\n",
      "Windows of missing data:\n",
      "Number of windows of missing data 5\n",
      "longest window of missing data: 5\n",
      "Point missing data 2\n",
      "station\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(names)):\n",
    "    print(names[i])\n",
    "    for n in df_arr[i].columns:\n",
    "        print(n)\n",
    "        cleaner.find_missing(df_arr[i], column = n)\n",
    "    print(\"=======================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yikes, looks like we've got tons of missing data in different columns\n",
    "### Following this, we have written a function to clean the data using\n",
    "\n",
    "1. **Combining data from highly correlated data sets**\n",
    "2. **Linear interpolation (only for POINT missing data; i.e [1,NULL,1] -> [1,1,1], [1,NULL,NULL,1] is not suitable)**\n",
    "3. **As a last resort, KNN imputation**\n",
    "\n",
    "Feel free to take a look a the source code to see exactly how we do it in **scripts/datacleaningutils.py**\n",
    "\n",
    "Note that our cleaner only can impute Numerical, Continuous variables.\n",
    "\n",
    "The Numeric Variables are ['year', 'month', 'day', 'hour', 'PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']\n",
    "\n",
    "Although year, month, day, hour are technically categories, there exists no missing data within those columns, hence they will not be affected.\n",
    "\n",
    "We also decided to ignore the station as we are going to create a combined dataset in the heart of beijing.\n",
    "\n",
    "The only categorical column we are going to look at is wd (wind direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arr_numeric_only = []\n",
    "for i in names:\n",
    "    df_arr_numeric_only.append(pd.read_csv(f\"data/{i}\")[['year', 'month', 'day', 'hour', 'PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arr_clean = cleaner.clean_data(df_arr_numeric_only) # this will clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRSA_Data_Dongsi_20130301-20170228.csv\n",
      "year\n",
      "month\n",
      "day\n",
      "hour\n",
      "PM2.5\n",
      "PM10\n",
      "SO2\n",
      "NO2\n",
      "CO\n",
      "O3\n",
      "TEMP\n",
      "PRES\n",
      "DEWP\n",
      "RAIN\n",
      "WSPM\n",
      "=======================\n",
      "PRSA_Data_Guanyuan_20130301-20170228.csv\n",
      "year\n",
      "month\n",
      "day\n",
      "hour\n",
      "PM2.5\n",
      "PM10\n",
      "SO2\n",
      "NO2\n",
      "CO\n",
      "O3\n",
      "TEMP\n",
      "PRES\n",
      "DEWP\n",
      "RAIN\n",
      "WSPM\n",
      "=======================\n",
      "PRSA_Data_Nongzhanguan_20130301-20170228.csv\n",
      "year\n",
      "month\n",
      "day\n",
      "hour\n",
      "PM2.5\n",
      "PM10\n",
      "SO2\n",
      "NO2\n",
      "CO\n",
      "O3\n",
      "TEMP\n",
      "PRES\n",
      "DEWP\n",
      "RAIN\n",
      "WSPM\n",
      "=======================\n",
      "PRSA_Data_Tiantan_20130301-20170228.csv\n",
      "year\n",
      "month\n",
      "day\n",
      "hour\n",
      "PM2.5\n",
      "PM10\n",
      "SO2\n",
      "NO2\n",
      "CO\n",
      "O3\n",
      "TEMP\n",
      "PRES\n",
      "DEWP\n",
      "RAIN\n",
      "WSPM\n",
      "=======================\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(names)):\n",
    "    print(names[i])\n",
    "    for n in df_arr_clean[i].columns:\n",
    "        print(n)\n",
    "        cleaner.find_missing(df_arr_clean[i], column = n)\n",
    "    print(\"=======================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just like that, we have successfully filled in all Numeric values.\n",
    "So what are we going to do about the Categorical Values?\n",
    "\n",
    "We are going to fill in the categorical values with the *mode*\n",
    "\n",
    "We will look at the most common win direction and use majority polling among the df_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ENE': 3, 'NE': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen = {}\n",
    "for i in range(len(names)):\n",
    "    mode = df_arr[i]['wd'].mode().values[0]\n",
    "    if mode not in seen:\n",
    "        seen[mode] = 1\n",
    "    else:\n",
    "        seen[mode] = seen[mode] + 1\n",
    "seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowing that ENE is the most common wind direction, we fill the missing wind directions into the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values found: 78\n",
      "After filling: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Null values found: {df_arr[0]['wd'].isnull().sum()}\")\n",
    "wd = df_arr[0]['wd'].fillna(\"ENE\")\n",
    "print(f\"After filling: {wd.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following this, we also merge all dataframes into the a single dataframe by taking the mean value between all numeric columns. This is one through our datacleaner library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cleaner.merge_dataframes_mean(df_arr_numeric_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almost there... Now we just need to add in the missing wd (wind direction column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['wd'] = wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>PRES</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>RAIN</th>\n",
       "      <th>WSPM</th>\n",
       "      <th>wd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>8.25</td>\n",
       "      <td>6.25</td>\n",
       "      <td>14.25</td>\n",
       "      <td>275.0</td>\n",
       "      <td>81.00</td>\n",
       "      <td>-0.550</td>\n",
       "      <td>1024.125</td>\n",
       "      <td>-20.750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.375</td>\n",
       "      <td>NNW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.50</td>\n",
       "      <td>12.25</td>\n",
       "      <td>6.75</td>\n",
       "      <td>14.00</td>\n",
       "      <td>275.0</td>\n",
       "      <td>81.00</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>1024.625</td>\n",
       "      <td>-21.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.100</td>\n",
       "      <td>NW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.75</td>\n",
       "      <td>5.50</td>\n",
       "      <td>7.25</td>\n",
       "      <td>15.50</td>\n",
       "      <td>275.0</td>\n",
       "      <td>71.75</td>\n",
       "      <td>-1.175</td>\n",
       "      <td>1024.850</td>\n",
       "      <td>-23.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.375</td>\n",
       "      <td>NNW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.25</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.25</td>\n",
       "      <td>17.00</td>\n",
       "      <td>325.0</td>\n",
       "      <td>70.50</td>\n",
       "      <td>-1.400</td>\n",
       "      <td>1025.775</td>\n",
       "      <td>-23.975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.450</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.75</td>\n",
       "      <td>6.25</td>\n",
       "      <td>16.00</td>\n",
       "      <td>300.0</td>\n",
       "      <td>75.50</td>\n",
       "      <td>-1.925</td>\n",
       "      <td>1026.625</td>\n",
       "      <td>-23.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.900</td>\n",
       "      <td>NNW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.75</td>\n",
       "      <td>39.25</td>\n",
       "      <td>2.75</td>\n",
       "      <td>30.25</td>\n",
       "      <td>425.0</td>\n",
       "      <td>69.50</td>\n",
       "      <td>12.500</td>\n",
       "      <td>1013.500</td>\n",
       "      <td>-16.200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.400</td>\n",
       "      <td>NW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.75</td>\n",
       "      <td>37.25</td>\n",
       "      <td>3.25</td>\n",
       "      <td>41.00</td>\n",
       "      <td>475.0</td>\n",
       "      <td>52.50</td>\n",
       "      <td>11.600</td>\n",
       "      <td>1013.600</td>\n",
       "      <td>-15.100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.900</td>\n",
       "      <td>WNW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>40.50</td>\n",
       "      <td>4.75</td>\n",
       "      <td>49.50</td>\n",
       "      <td>575.0</td>\n",
       "      <td>41.75</td>\n",
       "      <td>10.800</td>\n",
       "      <td>1014.200</td>\n",
       "      <td>-13.300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.100</td>\n",
       "      <td>NW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>38.50</td>\n",
       "      <td>6.25</td>\n",
       "      <td>55.75</td>\n",
       "      <td>625.0</td>\n",
       "      <td>35.25</td>\n",
       "      <td>10.500</td>\n",
       "      <td>1014.400</td>\n",
       "      <td>-12.900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.200</td>\n",
       "      <td>NNW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35063</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.50</td>\n",
       "      <td>44.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>775.0</td>\n",
       "      <td>24.25</td>\n",
       "      <td>8.600</td>\n",
       "      <td>1014.100</td>\n",
       "      <td>-15.900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.300</td>\n",
       "      <td>NNE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35064 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         year  month   day  hour  PM2.5   PM10   SO2    NO2     CO     O3  \\\n",
       "0      2013.0    3.0   1.0   0.0   6.00   8.25  6.25  14.25  275.0  81.00   \n",
       "1      2013.0    3.0   1.0   1.0   5.50  12.25  6.75  14.00  275.0  81.00   \n",
       "2      2013.0    3.0   1.0   2.0   4.75   5.50  7.25  15.50  275.0  71.75   \n",
       "3      2013.0    3.0   1.0   3.0   4.25   5.00  5.25  17.00  325.0  70.50   \n",
       "4      2013.0    3.0   1.0   4.0   4.00   4.75  6.25  16.00  300.0  75.50   \n",
       "...       ...    ...   ...   ...    ...    ...   ...    ...    ...    ...   \n",
       "35059  2017.0    2.0  28.0  19.0  15.75  39.25  2.75  30.25  425.0  69.50   \n",
       "35060  2017.0    2.0  28.0  20.0  16.75  37.25  3.25  41.00  475.0  52.50   \n",
       "35061  2017.0    2.0  28.0  21.0  18.00  40.50  4.75  49.50  575.0  41.75   \n",
       "35062  2017.0    2.0  28.0  22.0  15.00  38.50  6.25  55.75  625.0  35.25   \n",
       "35063  2017.0    2.0  28.0  23.0  17.50  44.00  7.00  64.00  775.0  24.25   \n",
       "\n",
       "         TEMP      PRES    DEWP  RAIN   WSPM   wd  \n",
       "0      -0.550  1024.125 -20.750   0.0  5.375  NNW  \n",
       "1      -0.800  1024.625 -21.125   0.0  4.100   NW  \n",
       "2      -1.175  1024.850 -23.000   0.0  5.375  NNW  \n",
       "3      -1.400  1025.775 -23.975   0.0  4.450    N  \n",
       "4      -1.925  1026.625 -23.250   0.0  2.900  NNW  \n",
       "...       ...       ...     ...   ...    ...  ...  \n",
       "35059  12.500  1013.500 -16.200   0.0  2.400   NW  \n",
       "35060  11.600  1013.600 -15.100   0.0  0.900  WNW  \n",
       "35061  10.800  1014.200 -13.300   0.0  1.100   NW  \n",
       "35062  10.500  1014.400 -12.900   0.0  1.200  NNW  \n",
       "35063   8.600  1014.100 -15.900   0.0  1.300  NNE  \n",
       "\n",
       "[35064 rows x 16 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Et Voila! We finally have our data cleaned! Next step is to export the data as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv(\"data/cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next step is to explore our data. This will be done in another notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
